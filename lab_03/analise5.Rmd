---
title: "Predição de Deputados Eleitos 2014"
author: "Gabriel Morais Lúcio de Araújo"
date: "28 de fevereiro de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(scales)
library(readr)
library(plotly)
library(caret)
library(rpart)
library(rpart.plot) 
```

```{r echo=FALSE}
trainK <- read.csv("~/Github/analiseDeDados/lab_03/train.csv", encoding = "latin1")
trainK <- trainK %>% select(-nome, -numero_cadidato, -ID)
trainK[is.na(trainK)] <- 0
testK <- read.csv("~/Github/analiseDeDados/lab_03/test.csv", encoding = "latin1")
testK <- testK %>% select(-nome, -numero_cadidato, -ID)
testK[is.na(testK)] <- 0
```

```{r}
## Kaggle
train.kaggle <- trainK
test.kaggle <- testK

## 75% of the sample size
smp_size <- floor(0.75 * nrow(train.kaggle))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(train.kaggle)), size = smp_size)

train <- train.kaggle[train_ind, ]
test <- train.kaggle[-train_ind, ]
```

### 1.Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador?

```{r}
train.kaggle.desbalanceamento.count <- train.kaggle %>%
  count(situacao_final)
train.kaggle.count.total <- train.kaggle %>%
  count()
nao_eleito.proportion <- 3719/4135
eleito.proportion <- 416/4135

train.kaggle %>%
  ggplot(aes(situacao_final)) + geom_histogram(stat="count")
```

Classe        | eleito       | nao_eleito  | Total
--------------|--------------|-------------|-------------
Instâncias    | 3719         | 416         | 4135
Proporção     | 0.1006       | 0.8993      | 1

Como podemos ver existe um desbalanceamento da classe situacao_final. Existem muito mais instâncias de **nao_eleito** do que de **eleito**, isso ira causar um desbalanceamento no classificador, que será mais tendencioso para **nao_eleito**. O classificador irá prever muito mais **nao_eleito**, poderiam ser utilizadas técnicas de balanceamento da clase. Por exemplo *Over Sampling*, onde serão inseridos dados da classe que possui menos instâncias, e *Under Sampling*, onde serão removidos dados da classe com mais instâncias. Usarei o *Under Sampling* nesta análise.

### 2.Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo. 

```{r warning=FALSE}
fitControlUndersampling <- trainControl(method = "repeatedcv", number = 5, repeats = 5, sampling="down")

preProcessing <- c("scale", "center", "nzv")

formula = as.formula(situacao_final ~ .)

modelo.glm <- train(formula,
                 data = train,
                 method="glm",
                 family="binomial",
                 trControl = fitControlUndersampling,
                 preProcess = preProcessing)

summary(modelo.glm)
```

Após executar o modelo glm, retirei as váriaveis que tinham graus de significância maior que **(0.001), deixando apenas as váriaveis mais significantes.

```{r}
formula = as.formula(situacao_final ~ total_despesa + descricao_ocupacao + media_receita + descricao_cor_raca + despesa_max_campanha)

modelo.arvore <- train(formula,
                       data= train,
                       method="rpart",
                       na.action = na.omit, 
                       trControl = fitControlUndersampling,
                       preProcess = preProcessing,
                       cp=0.001,
                       maxdepth=20)

modelo.arvore
```

```{r}
modelo.adaboost <- train(formula,
                         data= train,
                         method="adaboost",
                         na.action = na.omit, 
                         preProcess = preProcessing)

modelo.adaboost
```

### 3.Reporte acurácia, precision, recall e f-measure no treino e validação. Como você avalia os resultados? Justifique sua resposta.

```{r}
# Predição
test$prediction <- predict(modelo.adaboost, test)

# Número de vezes que o modelo acertou um candidato eleito
acerto.eleito <- test %>% filter(situacao_final == "eleito", prediction == "eleito") %>% nrow()

# Número de vezes que o modelo não acertou um candidato eleito
erro.eleito <- test %>% filter(situacao_final == "nao_eleito" , prediction == "eleito") %>% nrow() 

# # Número de vezes que o modelo acertou um candidato não_eleito
acerto.n_eleito <- test %>% filter(situacao_final == "nao_eleito" , prediction == "nao_eleito" ) %>% nrow()

# Número de vezes que o modelo não acertou um candidato não_eleito
erro.n_eleito <- test %>% filter(situacao_final == "eleito", prediction == "nao_eleito" ) %>% nrow()

accuracy <- (acerto.eleito + acerto.n_eleito)/(acerto.eleito + acerto.n_eleito + erro.eleito + erro.n_eleito) 
precision <- acerto.eleito / (acerto.eleito + erro.eleito)
recall <- acerto.eleito / (acerto.eleito + erro.n_eleito)

# média harmônica da precisão e recall
f_measure <- 2 * (precision * recall) / (precision + recall)

f_measure
accuracy
precision
recall

confusionMatrix(test$prediction, test$situacao_final)
```

Na validação, o valor da acurácia foi 0.9207. É um resultado satisfatório, pois indica que o modelo conseguiu realizar a predição do resultado da eleição na maioria dos casos. O f-measure é medido calculando a média harmônica entre precisão e recall, ter valor próximo a 1 é um bom indicador que o modelo é um bom preditor.

### 4.Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo? Crie pelo menos um novo atributo que não está nos dados originais e estude o impacto desse atributo.

```{r}
# Train
train$isDeputado <- ifelse(train$descricao_ocupacao == 'DEPUTADO', 1, 0)
# Test
test$isDeputado <- ifelse(test$descricao_ocupacao == 'DEPUTADO', 1, 0)
formula = as.formula(situacao_final ~ total_despesa + isDeputado + media_receita + descricao_cor_raca + despesa_max_campanha)
```

Foram criado os atributos isDeputado (1 = DEPUTADO, 0 = Caso contrário) e isHomem (1 = HOMEM, 0 = Caso contrário). Vamos avaliar o impacto desses atributos nos modelos a seguir.

* Regressão Logística
```{r}
# Antes dos novos atributos
summary(modelo.glm)

# Treino com novos atributos
modelo.glm2 <- train(formula,
                 data = train,
                 method="glm",
                 family="binomial",
                 trControl = fitControlUndersampling,
                 preProcess = preProcessing)

# Depois dos novos atributos
summary(modelo.glm2)
```

No modelo de Regressão Logística a inserção do atributo isDeputado teve relevância para o modelo, como mostrado acima.

* Árvore
```{r}
# Antes dos novos atributos
varImp(modelo.arvore)

# Treino com novos atributos
modelo.arvore2 <- train(formula,
                       data= train,
                       method="rpart",
                       na.action = na.omit, 
                       trControl = fitControlUndersampling,
                       preProcess = preProcessing,
                       cp=0.001,
                       maxdepth=20)

# Depois dos novos atributos
varImp(modelo.arvore2)
```

Para a Árvore de Decisões, os atributos *total_despesa, media_receita e isDeputado* parecem ser mais importantes. A inserção da variável isDeputado não aumentou a acurácia de forma significante.

* Adaboost
```{r}
# Antes dos novos atributos
varImp(modelo.adaboost)

# Treino com novos atributos
modelo.adaboost2 <- train(x = train[, names(train) != "situacao_final"],
                y = train$situacao_final,
                method = "adaboost",
                trControl = fitControl)

# Depois dos novos atributos
varImp(modelo.adaboost2)
```

Para o Adaboost, *total_receita, total_despesa, quantidade_despesas, quantidade_fornecedores, quantidade_doacoes, media_receita, quantidade_doadores, recursos_de_pessoas_físicas, recursos_de_pessoas_juridicas, recursos_de_partidos, e media_despesa* foram as variáveis mais importantes, o novo atributo isDeputados não apresenteu muita importância para o modelo.
```{r}
newCSV <- ID
newCSV$prediction <- test$prediction
write.csv(newCSV, "sample_submission.csv", row.names=FALSE)
```
