---
title: "Deep Learning em R com Keras"
author: "Gabriel Morais Lúcio de Araújo"
date: "March 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
```

Olá! Neste tutorial iremos utilizar Deep Learning no R com o pacote *keras*. **Deep Learning** é um campo de estudo derivado de Aprendizado de Máquina (Machine Learning) e consiste de algoritmos que são inspirados pelo funcionamento e a estrutura do cerébro.

### Sumário

* Visão geral sobre pacotes de Deep Learning para R
* Keras, keras e KerasR
* Instalando o pacote keras
* Carregando os dados
* Explorando os dados
* Processando os dados
* Construindo o modelo
* Compilando e ajustando o modelo
* Visualizando o histórico do modelo
* Predizendo valores
* Avaliando o modelo
* Tuning do modelo
* Salvando, carregando e exportando o modelo
* Conclusão

### **Visão geral sobre pacotes de Deep Learning para R**
Com o aumento da popularidade de Deep Learning o número de pacotes que o implementam aumentaram. Vejamos abaixo uma tabela com os pacotes mais utilizados para R:

R Package	 | Percentile	| Description
-----------|------------|-------------------------------------------------------------------------------------------------------
nnet	     | 96th	      | Software for feed-forward neural networks with a single hidden layer, and for multinomial log-linear models.
neuralnet	 | 96th	      | Training of neural networks using backpropagation
h2o	       | 95th	      | R scripting functionality for H2O
RSNNS	     | 88th	      | Interface to the Stuttgart Neural Network Simulator (SNNS)
tensorflow | 88th	      | Interface to TensorFlow
deepnet	   | 84th	      | Deep learning toolkit in R
darch	     | 79th	      | Package for Deep Architectures and Restricted Boltzmann Machines
rnn	       | 73rd	      | Package to implement Recurrent Neural Networks (RRNs)
FCNN4R	   | 52nd	      | Interface to the FCNN library that allows user-extensible ANNs
rcppDL	   | 7th	      | Implementation of basic machine learning methods with many layers (deep learning), including dA (Denoising Autoencoder), SdA (Stacked Denoising Autoencoder), RBM (Restricted Boltzmann machine) and DBN (Deep Belief Nets)
deepr	     | ??*	      | Package to streamline the training, fine-tuning and predicting processes for deep learning based on darch and deepnet
MXNetR	   | ??*	      | Package that brings flexible and efficient GPU computing and state-of-art deep learning to R

> ###### Os pacotes *deepr* e *MXNetR* não foram encontrados na documentação do R, por isso os percentis não forum incluídos na tabela.

[Fonte](https://cran.r-project.org/view=MachineLearning)

### **Keras, keras e KerasR**
Recentemente dois pacotes chegaram para a biblioteca R, o *KerasR* e o pacote para RStudio *keras*. Ambos provêm uma interface do pacote original *Keras* para **Python**. Interface significa que esses pacotes possibilitam um desenvolvimento utilizando a linguagem R, porém tendo acesso a praticamente todas as funcionalidades que o pacote *Keras* do python tem a oferecer!

As diferenças básicas entre os pacotes *KerasR* e *keras* são:

* O pacote *keras* utiliza o operador **pipe** (%>%) para conectar funções e operadores, assim aumentando muito a legibilidade do código.
* O pacote *KerasR* possui algumas funções com nome diferente das funçõe do pacote *Keras*. Isso talvez seja uma dificuldade, pois existem muitas discussões e tutoriais que utilizam o pacote *Keras* do python que poderiam ser aplicados ao pacote *keras*.

Com base nestes pontos, iremos utilizar o pacote *keras* do RStudio. Então vamos para a sua instalação!

### **Instalando o pacote keras**
Primeiramente abra o **RStudio** e vá na aba `Tools` no menu superior, em seguida clique em `Install Packages...` e pesquise pelo pacote `keras`. Quando o pacote terminar de ser instalado, adicione o pacote no seu arquivo, para isso basta adicionar `library(keras)` em um bloco de código R no ínicio do arquivo.

Por último instale e adicione o pacote `tensorflow` seguindo os mesmos passos de instalção ditos anteriormente.

> ###### Caso encontre algum problema veja os passos descritos no site oficial do pacote [aqui](https://rstudio.github.io/keras/)

### **Carregando os dados**
Para carregar os dados você tem **três** opções:

1. **Utilizar um dos *datasets* que o `keras` oferece**
    + O keras possui *datasets* que vem com a instalação do pacote. Por exemplo ele disponibiliza *datasets* para MNIST, CIFAR10 e IMDB. Eles podem ser carregados com `mnist <- dataset_mnist()`, `cifar10 <- dataset_cifar10()` ou `imdb <- dataset_imdb()`, respectivamente.
    
2. **Utilizar o seu próprio *dataset*, por exemplo a partir de arquivos *.csv***
    + Neste tutorial iremos utilizar a opção de carregar um arquivo *.csv* com o comando `read.csv()`. Iremos obter os dados a partir do [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php).
    + Para obter os dados execute `iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)`. Em seguida é importante checar se a importação dos dados foi bem sucedida, para isso execute `head(iris)`, `str(iris)` e `dim(iris)`.
    
3. **Ou produzir um *dummy dataset***
    + De forma alternativa você pode criar seus próprios dados com ajuda co comando `matrix()`. Por exemplo `data <- matrix(rexp(1000*784), nrow = 1000, ncol = 784)` para criar o dummy data e `labels <- matrix(round(runif(1000*10, min = 0, max = 9)), nrow = 1000, ncol = 10)` para criar valores alvos para seu dummy data criado anteriormente. Em seguida é muito importante analisar os dados gerados, pois conhecer seus dados antes de analisá-los ajuda muito na sua compreensão.
    
```{r}
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)

head(iris)

str(iris)

dim(iris)
```


### **Explorando os dados**
Agora com os dados importados é importante saber o que eles são. Vendo a imagem abaixo, iremos tentar diferenciar os três tipos de iris com base nos dados.
![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/iris-machinelearning.png)

A importação com `read.csv()` gera um *dataframe*, mas para podermos utilizar a função `fit()` teremos que utilizar os dados em formato de *array* ou *matrix*, ambos não possuem nomes para as colunas.

Por enquanto colunas com nomes podem ajudar no entendimento dos dados, então vamos nomea-las, pois os dados importados estão com colunas com nomes como V1, V2 ... Execute o bloco de código abaixo:

```{r}
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")

plot(iris$Petal.Length, 
     iris$Petal.Width, 
     pch=21, bg=c("red","green3","blue")[unclass(iris$Species)], 
     xlab="Petal Length", 
     ylab="Petal Width")
```

Analisando de forma rápida é possível notar uma correlação entre a largura e o tamanho das petálas. Para confirmar e obter um valor númerico sobre isso utilize a fução `cor()`:

```{r}
cor(iris$Petal.Length, iris$Petal.Width)
```

### **Processando os dados**
Antes de gerar um modelo, precisamos processar os dados. Isto é limpar, normalizar (se necessário) e dividir os dados em treino e teste.

Os dados obtidos do [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php) já estão limpos, mas se quiser verificar execute o código abaixo:

```{r}
summary(iris)

str(iris)
```

Agora vamos analisar se os dados precisam ser **Normalizados**. A partir dos resultados obtidos com a função `summary()` podemos ver que o atributo Sepal.Lenght varia entre 4.3 e 7.3 e Sepal.Width varia entre 2 e 4.4, enquanto Petal.Lenght varia entre 1 e 6.9 e Petal.Width varia entre 0.1 e 2.5. No geral, todos os valores estão no intervalor de 0.1 e 7.9, o que é considerado aceitável e assim os dados não precisam ser normalizados.

Porém, caso você precise normalizar outros *datasets* veja os exemplos abaixo de normalização com uma função própria e outra forma utilzando o `keras`.

* Você pode criar sua própria função de normalização, neste exemplo irei utilizar uma função de `min-max` para realizar essa tarefa, veja abaixo:
```{r}
# Build your own `normalize()` function
normalize <- function(x) {
  num <- x - min(x)
  denom <- max(x) - min(x)
  return (num/denom)
}

# Normalize the `iris` data
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))

head(iris)
```

* Ou você pode utilizar as funções do keras para isso:
```{r}
# Nomarlizando os dados iris
iris <- normalize(iris[,1:4])

summary(iris)
```

Por último vamos criar nossos dados de treino e de testes, para assegurar que o modelo irá conseguir prever dados que estão fora dos dados que ele foi treinado.

```{r}
# Determinando do tamanho dos sets
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))

# Dividindo o dataset iris
iris.training <- iris[ind==1, 1:4]
iris.test <- iris[ind==2, 1:4]

# Atribuindo os dados de treino e teste
iris.trainingtarget <- iris[ind==1, 5]
iris.testtarget <- iris[ind==2, 5]
```

Como último passo, devemos transformar os dados em uma matriz com um valor booleano para cada classe de valor e se essa classe possuem um valor. Para isso o pacote `keras` também nos ajuda:
```{r}
# Variável alvo de treino
iris.trainLabels <- to_categorical(iris.trainingtarget)

# Variável alvo de teste
iris.testLabels <- to_categorical(iris.testtarget)

print(iris.testLabels)
```

Agora podemos começar a construir o modelo!

### **Construindo o modelo**
Primeiramente precisamos de um modelo sequencial que pode ser gerado pelo `keras` com `keras_model_sequential()`. Antes disso, devemos notar que trabalhar com dados númericos é mais fácil, porém já realizamos o processamento para obter uma variável alvo binária indicando qual o tipo da flor. Uma rede que possui bom desemepenho para esse tipo de problema é a multi-layer (várias camadas) perceptron. É um tipo de rede que é geralmente completamente conectada. Para isso utilizaremos a função de ativação 'relu' e 'softmax' para garantir que os valores de predição estejam entre 1 e 0.
```{r}
# Modelo sequencial
model <- keras_model_sequential() 

# Adicionando camadas ao modelo.
model %>% 
    layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>% 
    layer_dense(units = 3, activation = 'softmax')
```

Abaixo algumas funções para você analisar o seu modelo de forma mais profunda:
```{r}
# Resumo do modelo
summary(model)

# Configuração do modelo
get_config(model)

# Configuração das camadas
get_layer(model, index = 1)

# Lista as camadas do modelo
model$layers

# Lista os tensors de entrada
model$inputs

# Lista os tensors de saída
model$outputs
```

### **Compilando e ajustando o modelo**
Para compilar nosso modelo, precisamos configurar o modelo com o otimizador `adam` e a função de perda `categorical_crossentropy`. Também adicionaremos a métrica `accuracy` para monitorar o modelo.
```{r}
# Compilando o modelo
model %>% compile(
     loss = 'categorical_crossentropy',
     optimizer = 'adam',
     metrics = 'accuracy'
 )
```
Agora vamos ajustar o modelo (fit) para 200 iterações com conjuntos de 5 amostras.
```{r}
# Ajuste o modelo
model %>% fit(
     iris.training, 
     iris.trainLabels, 
     epochs = 200, 
     batch_size = 5, 
     validation_split = 0.2
 )
```

### **Visualizando o histórico do modelo**
É uma boa ideia visualizar o histórico de ajuste do modelo, para melhor analisar as mudanças.
```{r}
# Store the fitting history in `history` 
history <- model %>% fit(
     iris.training, 
     iris.trainLabels, 
     epochs = 200,
     batch_size = 5, 
     validation_split = 0.2
 )

# Plot the history
plot(history)
```

Os gráficos gerados podem ser dificeis de analisar. Para facilitar um pouco, vamos *plotar* gráficos para os dados de treino e de teste. Primeiro o gráfico para a perda.
```{r}
# Plot the model loss of the training data
plot(history$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", col="blue", type="l")

# Plot the model loss of the test data
lines(history$metrics$val_loss, col="green")

# Add legend
legend("topright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
```
Agora para a taxa de acerto.
```{r}
# Plot the accuracy of the training data 
plot(history$metrics$acc, main="Model Accuracy", xlab = "epoch", ylab="accuracy", col="blue", type="l")

# Plot the accuracy of the validation data
lines(history$metrics$val_acc, col="green")

# Add Legend
legend("bottomright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
```
* Se sua taxa de acerto de treino aumenta enquanto a taxa de acerto do teste diminui, isso indica que está acontecendo um *overfitting* e que seu modelo está apenas memorizando os dados de treino.
* Se ambas taxa de acerto estão crescendo nas última iterações, indica que o modelo ainda poderia sofrer mais iterações antes de poder sofrer de *overfitting*.

### **Predizendo valores**
Agora que o modelo já foi treinado, é hora de predizer valores a partir dos dados de teste. Para isso podemos utilizar a função `predict()`. Depois disso podemos usar a função `table()` para fazer uma matriz de confursão dos valores de predição e os dados de teste.
```{r}
# Predict the classes for the test data
classes <- model %>% predict_classes(iris.test, batch_size = 128)

# Confusion matrix
table(iris.testtarget, classes)
```

### **Avaliando o modelo**
### **Tuning do modelo**
### **Salvando, carregando e exportando o modelo**
### **Conclusão**